[package]
name = "yai-mind"
version = "0.1.0"
edition = "2021"
authors = ["Francesco Maiomascio"]
description = "YAI Cognitive Core (Mind Layer - L3)"

[features]
# Default: build snella. Niente ONNX, niente C++ (usearch), niente HTTP.
default = ["minimal"]

# Core runtime (storage + graph + ids + json)
minimal = []

# --- Optional / heavy ---
http-client = ["dep:reqwest"]

vector-usearch = ["dep:usearch"]

embeddings-onnx = ["dep:tract-onnx", "dep:tokenizers"]
embeddings-candle = [
  "dep:candle-core",
  "dep:candle-nn",
  "dep:candle-transformers",
  "dep:tokenizers"
]

# Legacy gates
legacy = []
legacy_cognition = []
legacy-providers = ["legacy"]

[dependencies]
anyhow = "1"

# Identificatori
uuid = { version = "1.7", features = ["v4", "fast-rng"] }

# Serializzazione
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# Storage & Graph
rusqlite = { version = "0.31", features = ["bundled"] }
petgraph = "0.6"

# Hashing (core)
blake3 = "1"

# Vector search (HEAVY: pulls cxx) -> optional
usearch = { version = "2", optional = true }

# Networking (HEAVY) -> optional
# Nota: tolgo "blocking": se ti serve davvero, meglio metterla sotto un'altra feature,
# ma intanto rendiamola opzionale.
reqwest = { version = "0.12", default-features = false, features = ["rustls-tls", "json"], optional = true }

# Optional embeddings backends
tokenizers = { version = "0.15", optional = true }
tract-onnx = { version = "0.20", optional = true }
candle-core = { version = "0.5", optional = true }
candle-nn = { version = "0.5", optional = true }
candle-transformers = { version = "0.5", optional = true }

[dev-dependencies]
memoffset = "0.9"

[[bin]]
name = "yai-mind"
path = "src/main.rs"
